
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/tensorclass_fashion.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_tutorials_tensorclass_fashion.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_tensorclass_fashion.py:


Using tensorclasses for datasets
================================

.. GENERATED FROM PYTHON SOURCE LINES 7-13

In this tutorial we demonstrate how tensorclasses can be used to
efficiently and transparently load and manage data inside a training
pipeline. The tutorial is based heavily on the `PyTorch Quickstart
Tutorial <https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html>`__,
but modified to demonstrate use of tensorclass. See the related tutorial using
``TensorDict``.

.. GENERATED FROM PYTHON SOURCE LINES 13-27

.. code-block:: Python



    import torch
    import torch.nn as nn

    from tensordict import MemoryMappedTensor, tensorclass
    from torch.utils.data import DataLoader
    from torchvision import datasets
    from torchvision.transforms import ToTensor

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Using device: cpu




.. GENERATED FROM PYTHON SOURCE LINES 28-32

The ``torchvision.datasets`` module contains a number of convenient pre-prepared
datasets. In this tutorial we'll use the relatively simple FashionMNIST dataset. Each
image is an item of clothing, the objective is to classify the type of clothing in
the image (e.g. "Bag", "Sneaker" etc.).

.. GENERATED FROM PYTHON SOURCE LINES 32-46

.. code-block:: Python


    training_data = datasets.FashionMNIST(
        root="data",
        train=True,
        download=True,
        transform=ToTensor(),
    )
    test_data = datasets.FashionMNIST(
        root="data",
        train=False,
        download=True,
        transform=ToTensor(),
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0.00/26.4M [00:00<?, ?B/s]      0%|          | 65.5k/26.4M [00:00<02:25, 181kB/s]      0%|          | 131k/26.4M [00:00<01:29, 293kB/s]       1%|          | 197k/26.4M [00:00<01:21, 321kB/s]      1%|          | 262k/26.4M [00:00<01:17, 336kB/s]      1%|          | 328k/26.4M [00:01<01:15, 345kB/s]      1%|▏         | 393k/26.4M [00:01<01:14, 351kB/s]      2%|▏         | 459k/26.4M [00:01<01:08, 379kB/s]      2%|▏         | 524k/26.4M [00:01<01:09, 374kB/s]      2%|▏         | 590k/26.4M [00:01<01:03, 409kB/s]      2%|▏         | 655k/26.4M [00:01<01:05, 394kB/s]      3%|▎         | 721k/26.4M [00:01<01:02, 410kB/s]      3%|▎         | 786k/26.4M [00:02<01:03, 404kB/s]      3%|▎         | 852k/26.4M [00:02<01:00, 422kB/s]      3%|▎         | 918k/26.4M [00:02<00:59, 431kB/s]      4%|▎         | 983k/26.4M [00:02<00:56, 454kB/s]      4%|▍         | 1.05M/26.4M [00:02<00:55, 454kB/s]      4%|▍         | 1.11M/26.4M [00:02<00:53, 471kB/s]      4%|▍         | 1.18M/26.4M [00:02<00:54, 466kB/s]      5%|▍         | 1.25M/26.4M [00:03<00:52, 480kB/s]      5%|▌         | 1.34M/26.4M [00:03<00:49, 503kB/s]      5%|▌         | 1.44M/26.4M [00:03<00:48, 517kB/s]      6%|▌         | 1.54M/26.4M [00:03<00:47, 526kB/s]      6%|▌         | 1.64M/26.4M [00:03<00:43, 568kB/s]      6%|▋         | 1.70M/26.4M [00:03<00:44, 555kB/s]      7%|▋         | 1.80M/26.4M [00:04<00:41, 592kB/s]      7%|▋         | 1.87M/26.4M [00:04<00:42, 572kB/s]      7%|▋         | 1.97M/26.4M [00:04<00:40, 605kB/s]      8%|▊         | 2.06M/26.4M [00:04<00:40, 597kB/s]      8%|▊         | 2.13M/26.4M [00:04<00:39, 607kB/s]      8%|▊         | 2.23M/26.4M [00:04<00:36, 654kB/s]      9%|▉         | 2.33M/26.4M [00:04<00:36, 664kB/s]      9%|▉         | 2.42M/26.4M [00:05<00:34, 694kB/s]     10%|▉         | 2.52M/26.4M [00:05<00:34, 691kB/s]     10%|▉         | 2.62M/26.4M [00:05<00:36, 654kB/s]     10%|█         | 2.72M/26.4M [00:05<00:32, 724kB/s]     11%|█         | 2.82M/26.4M [00:05<00:31, 738kB/s]     11%|█         | 2.92M/26.4M [00:05<00:32, 722kB/s]     11%|█▏        | 3.01M/26.4M [00:05<00:31, 735kB/s]     12%|█▏        | 3.11M/26.4M [00:05<00:32, 720kB/s]     12%|█▏        | 3.21M/26.4M [00:06<00:31, 734kB/s]     13%|█▎        | 3.34M/26.4M [00:06<00:29, 791kB/s]     13%|█▎        | 3.44M/26.4M [00:06<00:29, 784kB/s]     13%|█▎        | 3.54M/26.4M [00:06<00:29, 773kB/s]     14%|█▍        | 3.64M/26.4M [00:06<00:27, 821kB/s]     14%|█▍        | 3.74M/26.4M [00:06<00:28, 804kB/s]     15%|█▍        | 3.87M/26.4M [00:06<00:26, 841kB/s]     15%|█▌        | 4.00M/26.4M [00:07<00:27, 817kB/s]     16%|█▌        | 4.13M/26.4M [00:07<00:26, 853kB/s]     16%|█▌        | 4.26M/26.4M [00:07<00:24, 894kB/s]     16%|█▋        | 4.36M/26.4M [00:07<00:24, 911kB/s]     17%|█▋        | 4.49M/26.4M [00:07<00:25, 862kB/s]     17%|█▋        | 4.62M/26.4M [00:07<00:22, 962kB/s]     18%|█▊        | 4.75M/26.4M [00:07<00:22, 980kB/s]     18%|█▊        | 4.88M/26.4M [00:07<00:21, 986kB/s]     19%|█▉        | 5.01M/26.4M [00:08<00:20, 1.06MB/s]     19%|█▉        | 5.14M/26.4M [00:08<00:20, 1.05MB/s]     20%|██        | 5.31M/26.4M [00:08<00:18, 1.11MB/s]     21%|██        | 5.44M/26.4M [00:08<00:18, 1.16MB/s]     21%|██        | 5.60M/26.4M [00:08<00:17, 1.19MB/s]     22%|██▏       | 5.77M/26.4M [00:08<00:17, 1.21MB/s]     23%|██▎       | 5.96M/26.4M [00:08<00:14, 1.40MB/s]     23%|██▎       | 6.13M/26.4M [00:08<00:14, 1.36MB/s]     24%|██▍       | 6.32M/26.4M [00:09<00:14, 1.40MB/s]     25%|██▍       | 6.55M/26.4M [00:09<00:12, 1.63MB/s]     26%|██▌       | 6.75M/26.4M [00:09<00:12, 1.59MB/s]     26%|██▋       | 6.98M/26.4M [00:09<00:11, 1.64MB/s]     27%|██▋       | 7.21M/26.4M [00:09<00:10, 1.81MB/s]     28%|██▊       | 7.44M/26.4M [00:09<00:10, 1.79MB/s]     29%|██▉       | 7.70M/26.4M [00:09<00:10, 1.86MB/s]     30%|███       | 8.00M/26.4M [00:09<00:08, 2.13MB/s]     31%|███▏      | 8.26M/26.4M [00:09<00:08, 2.10MB/s]     32%|███▏      | 8.55M/26.4M [00:10<00:08, 2.15MB/s]     34%|███▎      | 8.88M/26.4M [00:10<00:07, 2.43MB/s]     35%|███▍      | 9.18M/26.4M [00:10<00:07, 2.38MB/s]     36%|███▌      | 9.50M/26.4M [00:10<00:06, 2.42MB/s]     37%|███▋      | 9.90M/26.4M [00:10<00:05, 2.80MB/s]     39%|███▊      | 10.2M/26.4M [00:10<00:06, 2.64MB/s]     40%|████      | 10.6M/26.4M [00:10<00:05, 2.82MB/s]     42%|████▏     | 11.0M/26.4M [00:10<00:04, 3.19MB/s]     43%|████▎     | 11.4M/26.4M [00:11<00:05, 2.98MB/s]     45%|████▍     | 11.8M/26.4M [00:11<00:04, 3.15MB/s]     47%|████▋     | 12.3M/26.4M [00:11<00:03, 3.59MB/s]     48%|████▊     | 12.7M/26.4M [00:11<00:04, 3.41MB/s]     50%|█████     | 13.2M/26.4M [00:11<00:03, 3.58MB/s]     52%|█████▏    | 13.8M/26.4M [00:11<00:03, 3.72MB/s]     54%|█████▍    | 14.4M/26.4M [00:11<00:03, 3.94MB/s]     57%|█████▋    | 15.0M/26.4M [00:11<00:02, 4.12MB/s]     59%|█████▊    | 15.5M/26.4M [00:12<00:03, 3.52MB/s]     62%|██████▏   | 16.3M/26.4M [00:12<00:02, 4.20MB/s]     64%|██████▎   | 16.8M/26.4M [00:12<00:02, 4.15MB/s]     65%|██████▌   | 17.2M/26.4M [00:12<00:02, 3.87MB/s]     67%|██████▋   | 17.8M/26.4M [00:12<00:02, 3.92MB/s]     69%|██████▉   | 18.2M/26.4M [00:12<00:02, 3.71MB/s]     71%|███████   | 18.8M/26.4M [00:12<00:01, 3.88MB/s]     73%|███████▎  | 19.2M/26.4M [00:13<00:01, 3.69MB/s]     75%|███████▌  | 19.9M/26.4M [00:13<00:01, 4.29MB/s]     77%|███████▋  | 20.3M/26.4M [00:13<00:01, 3.70MB/s]     79%|███████▉  | 21.0M/26.4M [00:13<00:01, 4.37MB/s]     81%|████████  | 21.5M/26.4M [00:13<00:01, 3.82MB/s]     84%|████████▎ | 22.1M/26.4M [00:13<00:00, 4.38MB/s]     85%|████████▌ | 22.6M/26.4M [00:13<00:00, 4.18MB/s]     87%|████████▋ | 23.0M/26.4M [00:14<00:00, 3.98MB/s]     90%|████████▉ | 23.7M/26.4M [00:14<00:00, 4.22MB/s]     92%|█████████▏| 24.2M/26.4M [00:14<00:00, 4.08MB/s]     93%|█████████▎| 24.6M/26.4M [00:14<00:00, 3.75MB/s]     95%|█████████▌| 25.2M/26.4M [00:14<00:00, 4.04MB/s]     97%|█████████▋| 25.7M/26.4M [00:14<00:00, 4.16MB/s]     99%|█████████▉| 26.1M/26.4M [00:14<00:00, 3.55MB/s]    100%|██████████| 26.4M/26.4M [00:14<00:00, 1.77MB/s]
      0%|          | 0.00/29.5k [00:00<?, ?B/s]    100%|██████████| 29.5k/29.5k [00:00<00:00, 327kB/s]
      0%|          | 0.00/4.42M [00:00<?, ?B/s]      1%|▏         | 65.5k/4.42M [00:00<00:11, 364kB/s]      5%|▌         | 229k/4.42M [00:00<00:06, 685kB/s]      19%|█▉        | 852k/4.42M [00:00<00:01, 2.44MB/s]     41%|████▏     | 1.84M/4.42M [00:00<00:00, 4.28MB/s]     84%|████████▎ | 3.70M/4.42M [00:00<00:00, 7.86MB/s]    100%|██████████| 4.42M/4.42M [00:00<00:00, 6.12MB/s]
      0%|          | 0.00/5.15k [00:00<?, ?B/s]    100%|██████████| 5.15k/5.15k [00:00<00:00, 65.0MB/s]




.. GENERATED FROM PYTHON SOURCE LINES 47-58

Tensorclasses are dataclasses that expose dedicated tensor methods over
its contents much like ``TensorDict``. They are a good choice when the
structure of the data you want to store is fixed and predictable.

As well as specifying the contents, we can also encapsulate related
logic as custom methods when defining the class. In this case we'll
write a ``from_dataset`` classmethod that takes a dataset as input and
creates a tensorclass containing the data from the dataset. We create
memory-mapped tensors to hold the data. This will allow us to
efficiently load batches of transformed data from disk rather than
repeatedly load and transform individual images.

.. GENERATED FROM PYTHON SOURCE LINES 58-80

.. code-block:: Python



    @tensorclass
    class FashionMNISTData:
        images: torch.Tensor
        targets: torch.Tensor

        @classmethod
        def from_dataset(cls, dataset, device=None):
            data = cls(
                images=MemoryMappedTensor.empty(
                    (len(dataset), *dataset[0][0].squeeze().shape), dtype=torch.float32
                ),
                targets=MemoryMappedTensor.empty((len(dataset),), dtype=torch.int64),
                batch_size=[len(dataset)],
                device=device,
            )
            for i, (image, target) in enumerate(dataset):
                data[i] = cls(images=image, targets=torch.tensor(target), batch_size=[])
            return data









.. GENERATED FROM PYTHON SOURCE LINES 81-84

We will create two tensorclasses, one each for the training and test data. Note that
we incur some overhead here as we are looping over the entire dataset, transforming
and saving to disk.

.. GENERATED FROM PYTHON SOURCE LINES 84-88

.. code-block:: Python


    training_data_tc = FashionMNISTData.from_dataset(training_data, device=device)
    test_data_tc = FashionMNISTData.from_dataset(test_data, device=device)








.. GENERATED FROM PYTHON SOURCE LINES 89-100

DataLoaders
----------------

We'll create DataLoaders from the ``torchvision``-provided Datasets, as
well as from our memory-mapped TensorDicts.

Since ``TensorDict`` implements ``__len__`` and ``__getitem__`` (and
also ``__getitems__``) we can use it like a map-style Dataset and create
a ``DataLoader`` directly from it. Note that because ``TensorDict`` can
already handle batched indices, there is no need for collation, so we
pass the identity function as ``collate_fn``.

.. GENERATED FROM PYTHON SOURCE LINES 100-113

.. code-block:: Python


    batch_size = 64

    train_dataloader = DataLoader(training_data, batch_size=batch_size)  # noqa: TOR401
    test_dataloader = DataLoader(test_data, batch_size=batch_size)  # noqa: TOR401

    train_dataloader_tc = DataLoader(  # noqa: TOR401
        training_data_tc, batch_size=batch_size, collate_fn=lambda x: x
    )
    test_dataloader_tc = DataLoader(  # noqa: TOR401
        test_data_tc, batch_size=batch_size, collate_fn=lambda x: x
    )








.. GENERATED FROM PYTHON SOURCE LINES 114-120

Model
-------

We use the same model from the
`Quickstart Tutorial <https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html>`__.


.. GENERATED FROM PYTHON SOURCE LINES 120-144

.. code-block:: Python



    class Net(nn.Module):
        def __init__(self):
            super().__init__()
            self.flatten = nn.Flatten()
            self.linear_relu_stack = nn.Sequential(
                nn.Linear(28 * 28, 512),
                nn.ReLU(),
                nn.Linear(512, 512),
                nn.ReLU(),
                nn.Linear(512, 10),
            )

        def forward(self, x):
            x = self.flatten(x)
            logits = self.linear_relu_stack(x)
            return logits


    model = Net().to(device)
    model_tc = Net().to(device)
    model, model_tc





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    (Net(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (linear_relu_stack): Sequential(
        (0): Linear(in_features=784, out_features=512, bias=True)
        (1): ReLU()
        (2): Linear(in_features=512, out_features=512, bias=True)
        (3): ReLU()
        (4): Linear(in_features=512, out_features=10, bias=True)
      )
    ), Net(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (linear_relu_stack): Sequential(
        (0): Linear(in_features=784, out_features=512, bias=True)
        (1): ReLU()
        (2): Linear(in_features=512, out_features=512, bias=True)
        (3): ReLU()
        (4): Linear(in_features=512, out_features=10, bias=True)
      )
    ))



.. GENERATED FROM PYTHON SOURCE LINES 145-151

Optimizing the parameters
---------------------------------

We'll optimise the parameters of the model using stochastic gradient descent and
cross-entropy loss.


.. GENERATED FROM PYTHON SOURCE LINES 151-176

.. code-block:: Python


    loss_fn = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
    optimizer_tc = torch.optim.SGD(model_tc.parameters(), lr=1e-3)


    def train(dataloader, model, loss_fn, optimizer):
        size = len(dataloader.dataset)
        model.train()

        for batch, (X, y) in enumerate(dataloader):
            X, y = X.to(device), y.to(device)

            pred = model(X)
            loss = loss_fn(pred, y)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if batch % 100 == 0:
                loss, current = loss.item(), batch * len(X)
                print(f"loss: {loss:>7f} [{current:>5d}/{size:>5d}]")









.. GENERATED FROM PYTHON SOURCE LINES 177-181

The training loop for our tensorclass-based DataLoader is very similar, we just
adjust how we unpack the data to the more explicit attribute-based retrieval offered
by the tensorclass. The ``.contiguous()`` method loads the data stored in the memmap
tensor.

.. GENERATED FROM PYTHON SOURCE LINES 181-267

.. code-block:: Python



    def train_tc(dataloader, model, loss_fn, optimizer):
        size = len(dataloader.dataset)
        model.train()

        for batch, data in enumerate(dataloader):
            X, y = data.images.contiguous(), data.targets.contiguous()

            pred = model(X)
            loss = loss_fn(pred, y)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if batch % 100 == 0:
                loss, current = loss.item(), batch * len(X)
                print(f"loss: {loss:>7f} [{current:>5d}/{size:>5d}]")


    def test(dataloader, model, loss_fn):
        size = len(dataloader.dataset)
        num_batches = len(dataloader)
        model.eval()
        test_loss, correct = 0, 0
        with torch.no_grad():
            for X, y in dataloader:
                X, y = X.to(device), y.to(device)

                pred = model(X)

                test_loss += loss_fn(pred, y).item()
                correct += (pred.argmax(1) == y).type(torch.float).sum().item()

        test_loss /= num_batches
        correct /= size

        print(
            f"Test Error: \n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \n"
        )


    def test_tc(dataloader, model, loss_fn):
        size = len(dataloader.dataset)
        num_batches = len(dataloader)
        model.eval()
        test_loss, correct = 0, 0
        with torch.no_grad():
            for batch in dataloader:
                X, y = batch.images.contiguous(), batch.targets.contiguous()

                pred = model(X)

                test_loss += loss_fn(pred, y).item()
                correct += (pred.argmax(1) == y).type(torch.float).sum().item()

        test_loss /= num_batches
        correct /= size

        print(
            f"Test Error: \n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \n"
        )


    for d in train_dataloader_tc:
        print(d)
        break

    import time

    t0 = time.time()
    epochs = 5
    for t in range(epochs):
        print(f"Epoch {t + 1}\n-------------------------")
        train_tc(train_dataloader_tc, model_tc, loss_fn, optimizer_tc)
        test_tc(test_dataloader_tc, model_tc, loss_fn)
    print(f"Tensorclass training done! time: {time.time() - t0: 4.4f} s")

    t0 = time.time()
    epochs = 5
    for t in range(epochs):
        print(f"Epoch {t + 1}\n-------------------------")
        train(train_dataloader, model, loss_fn, optimizer)
        test(test_dataloader, model, loss_fn)
    print(f"Training done! time: {time.time() - t0: 4.4f} s")




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    FashionMNISTData(
        images=Tensor(shape=torch.Size([64, 28, 28]), device=cpu, dtype=torch.float32, is_shared=False),
        targets=Tensor(shape=torch.Size([64]), device=cpu, dtype=torch.int64, is_shared=False),
        batch_size=torch.Size([64]),
        device=cpu,
        is_shared=False)
    Epoch 1
    -------------------------
    loss: 2.298487 [    0/60000]
    loss: 2.288889 [ 6400/60000]
    loss: 2.268152 [12800/60000]
    loss: 2.269307 [19200/60000]
    loss: 2.243432 [25600/60000]
    loss: 2.217239 [32000/60000]
    loss: 2.230787 [38400/60000]
    loss: 2.194348 [44800/60000]
    loss: 2.183752 [51200/60000]
    loss: 2.163590 [57600/60000]
    Test Error: 
     Accuracy: 47.1%, Avg loss: 2.147551 

    Epoch 2
    -------------------------
    loss: 2.149502 [    0/60000]
    loss: 2.144371 [ 6400/60000]
    loss: 2.081595 [12800/60000]
    loss: 2.107840 [19200/60000]
    loss: 2.044401 [25600/60000]
    loss: 1.983332 [32000/60000]
    loss: 2.014694 [38400/60000]
    loss: 1.933643 [44800/60000]
    loss: 1.933015 [51200/60000]
    loss: 1.864164 [57600/60000]
    Test Error: 
     Accuracy: 59.3%, Avg loss: 1.858910 

    Epoch 3
    -------------------------
    loss: 1.883127 [    0/60000]
    loss: 1.855341 [ 6400/60000]
    loss: 1.736682 [12800/60000]
    loss: 1.791130 [19200/60000]
    loss: 1.672171 [25600/60000]
    loss: 1.622951 [32000/60000]
    loss: 1.645631 [38400/60000]
    loss: 1.556260 [44800/60000]
    loss: 1.570557 [51200/60000]
    loss: 1.472003 [57600/60000]
    Test Error: 
     Accuracy: 63.1%, Avg loss: 1.491125 

    Epoch 4
    -------------------------
    loss: 1.549017 [    0/60000]
    loss: 1.517798 [ 6400/60000]
    loss: 1.372352 [12800/60000]
    loss: 1.455083 [19200/60000]
    loss: 1.339631 [25600/60000]
    loss: 1.329374 [32000/60000]
    loss: 1.342476 [38400/60000]
    loss: 1.279428 [44800/60000]
    loss: 1.301826 [51200/60000]
    loss: 1.212701 [57600/60000]
    Test Error: 
     Accuracy: 63.7%, Avg loss: 1.234995 

    Epoch 5
    -------------------------
    loss: 1.305314 [    0/60000]
    loss: 1.287739 [ 6400/60000]
    loss: 1.125332 [12800/60000]
    loss: 1.241607 [19200/60000]
    loss: 1.125468 [25600/60000]
    loss: 1.137019 [32000/60000]
    loss: 1.158329 [38400/60000]
    loss: 1.108312 [44800/60000]
    loss: 1.133340 [51200/60000]
    loss: 1.061506 [57600/60000]
    Test Error: 
     Accuracy: 64.7%, Avg loss: 1.076493 

    Tensorclass training done! time:  8.3552 s
    Epoch 1
    -------------------------
    loss: 2.305974 [    0/60000]
    loss: 2.291361 [ 6400/60000]
    loss: 2.272344 [12800/60000]
    loss: 2.267220 [19200/60000]
    loss: 2.248962 [25600/60000]
    loss: 2.224437 [32000/60000]
    loss: 2.229744 [38400/60000]
    loss: 2.200737 [44800/60000]
    loss: 2.196026 [51200/60000]
    loss: 2.168514 [57600/60000]
    Test Error: 
     Accuracy: 51.1%, Avg loss: 2.161120 

    Epoch 2
    -------------------------
    loss: 2.175843 [    0/60000]
    loss: 2.160054 [ 6400/60000]
    loss: 2.102742 [12800/60000]
    loss: 2.117307 [19200/60000]
    loss: 2.063141 [25600/60000]
    loss: 2.014492 [32000/60000]
    loss: 2.038411 [38400/60000]
    loss: 1.963096 [44800/60000]
    loss: 1.964262 [51200/60000]
    loss: 1.896003 [57600/60000]
    Test Error: 
     Accuracy: 51.7%, Avg loss: 1.887402 

    Epoch 3
    -------------------------
    loss: 1.930198 [    0/60000]
    loss: 1.891744 [ 6400/60000]
    loss: 1.769839 [12800/60000]
    loss: 1.812620 [19200/60000]
    loss: 1.696914 [25600/60000]
    loss: 1.656157 [32000/60000]
    loss: 1.682766 [38400/60000]
    loss: 1.583109 [44800/60000]
    loss: 1.608384 [51200/60000]
    loss: 1.509497 [57600/60000]
    Test Error: 
     Accuracy: 58.1%, Avg loss: 1.517505 

    Epoch 4
    -------------------------
    loss: 1.595404 [    0/60000]
    loss: 1.552787 [ 6400/60000]
    loss: 1.399166 [12800/60000]
    loss: 1.474885 [19200/60000]
    loss: 1.348224 [25600/60000]
    loss: 1.346231 [32000/60000]
    loss: 1.364698 [38400/60000]
    loss: 1.290759 [44800/60000]
    loss: 1.324080 [51200/60000]
    loss: 1.231170 [57600/60000]
    Test Error: 
     Accuracy: 62.5%, Avg loss: 1.250974 

    Epoch 5
    -------------------------
    loss: 1.337284 [    0/60000]
    loss: 1.313384 [ 6400/60000]
    loss: 1.144640 [12800/60000]
    loss: 1.254603 [19200/60000]
    loss: 1.122362 [25600/60000]
    loss: 1.145913 [32000/60000]
    loss: 1.169767 [38400/60000]
    loss: 1.111341 [44800/60000]
    loss: 1.148063 [51200/60000]
    loss: 1.071522 [57600/60000]
    Test Error: 
     Accuracy: 64.4%, Avg loss: 1.087379 

    Training done! time:  34.6256 s





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (1 minutes 13.335 seconds)


.. _sphx_glr_download_tutorials_tensorclass_fashion.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tensorclass_fashion.ipynb <tensorclass_fashion.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tensorclass_fashion.py <tensorclass_fashion.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: tensorclass_fashion.zip <tensorclass_fashion.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
